Note: Random forest and logistic regression data files were too large to upload to github. They are available by google drive.

https://drive.google.com/drive/folders/1t8MgnFZVJRSFZC1a2W44LhSzpTsj1I6J
This Google Drive contains four components:
1) The R files for all statistical tasks (.R files in the main folder)
2) The rds and RData files generated by 01_functions.R and 02_data_wrangling.R (.RData and .rds files in main folder)
2) The dataset provided by Kaggle for training (train.csv in the main folder)
3) The data files generated by the other R files, organized into folders (the folders in the Google Drive)

The R Files do not need to necessarily be run in order, but the first two numbered files need to be run before the other ones. However, most of these files take a VERY LONG time to run (hours and hours).

What follows are descriptions of each of the R files

-01_functions.R
This file contains two functions for CV and training/holdout evaluation. It saves these functions in an RData file (functions.RData)

-02_data_wrangling.R
This file applies data wrangling and transformations to the dataset, and saves the dataset with transformations and modifications as a new file for analysis as an RDS file (transformed_dataset.rds). Requires test.csv to be present in the working directory.

-03_logistic_1.R
This file applies multinomial logistic regression to the dataset, first with training and holdout splits, and then with CV analysis on the top candidates. The RDS and RData files generated by this file are saved in the logistic_data_files folder. This file takes many hours to run.
The RDS files in the logistic_data_files folder generated by this file contain CV evaluation statistics for top performing models, as documented in 03_logistic_1.R. Similarly, holdout_evaluation.RData is also generated by this file, and contains the environment with training/holdout split evaluation models and their statistics.

-04_logistic_2.R
This file applies an additional experimental multinomial logistic regression to the dataset, that was unexpectedly well performing. This file produces interpret_logit.RData, which contains all fitted model objects and their evaluation statistics, and is saved in the logistic_data_files folder. This file takes many hours to run.

-05_xgboost.R
This file applies tree-based gradient boosting to the dataset, first with training and holdout splits, and then with CV analysis on the top candidate to find the best tuning paramesters. This file produces two files: the RData file xgboost.RData, which contains all fitted model objects and their evaluation statistics, and cv_results.csv, which contains this cross validation results during parameter tuning. These files are saved in the xgboost_data_files folder. This file takes many hours to run.

-06_classification_trees.R
contains holdout and training set data based on transfored_dataset.rds <<this file must be in the directory

varcomb function takes a data frame and fits an rpart model on every combination of variables.
This function takes multiple hours to run. The outputs are saved in class_trees_obj.RData
One is named cvs, this is the output on the log transformed data. The other is named cvsbin, made on the binned data. 

Running the code on 06_classification_trees.R should generate 3 plots - the tree diagram of the best fitted model, and a graph showing the performance of various pruning values. The console output will show classification tables at 50% and 80% prediction interval levels, as well as class specific misclassification rate for the best found classification tree model. 

class_trees_obj.RData also contains the best found classification tree model, fitted on a training set of 10 000 observations. 

-07_randomForest.R
This file fits random forest models, aggregates the performance results based on CV in one R object and saves the best random forest model. This model produces rds files, bestFR.rds and randomForestResults.rds, which document the best random forest model object and the evaluation statistics for all models evaluated, and are saved in the randomForest_data_files folder.This file takes about 50 minutes to run.

